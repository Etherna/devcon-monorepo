{
  "id": "beyond-ligero-and-brakedown-building-a-fast-prover-based-on-list-polynomial-commitments",
  "sourceId": "L7YER9",
  "title": "Beyond Ligero and Brakedown: Building a Fast Prover Based on List-Polynomial Commitments",
  "description": "Linear codes underlie one of the main approaches in zero-knowledge proofs and arguments, including works like FRI, Ligero, Brakedown and Orion. In this talk, we describe how to extend one of the protocols from Ligero and Brakedown to the regime of batched polynomial commitments, at the cost of a single extra operation in the verifier. Similarly to Redshift, we opt for increased efficiency via the list decoding regime. We also present an optimisation for using the resulting commitment with PIOPs.",
  "track": "Applied Cryptography",
  "type": "Talk",
  "expertise": "Intermediate",
  "audience": "Academic",
  "featured": false,
  "doNotRecord": false,
  "tags": [
    "Layer 2s",
    "Rollups",
    "Zero-Knowledge",
    "Cryptography",
    "Security",
    "provable",
    "Cryptography",
    "Layer 2s",
    "Rollups",
    "Zero-Knowledge"
  ],
  "keywords": [
    "Provable",
    "Security"
  ],
  "duration": 1518,
  "language": "en",
  "sources_swarmHash": "35f25ffc9ab45f0268459c63c3352846e73f664bd476c7e32502f24f3e3bd539",
  "sources_youtubeId": "V7hmwJ-l0qY",
  "sources_ipfsHash": "",
  "sources_livepeerId": "",
  "sources_streamethId": null,
  "transcript_vtt": "https://streameth-develop.ams3.digitaloceanspaces.com/transcriptions/67341fe49dbb7a90e16f03c3.vtt",
  "transcript_text": " Hi, everyone. So today we're going to talk about how to build provers. This is a joint talk with my colleague, Azam. And yeah, the main purpose of the talk is to see how we can use these polynomial commitments. And we're going to create something based on Ligero and Breakdown, which are two previous works. So just a quick word about us. We're both cryptography researchers at ConsenSys, and we're working in the Linear Prover team. And since you're here, you probably have been hearing all this time how to make proofs, how to make proofs, you want to make proofs, and this talk, this intermediate cryptography talk is about how to actually do that mathematically and how to construct a proof system. So we're going to dig into some details, some mathematical details, but we're going to do that hopefully in a way that carries information across to you. So just some motivation. At the high level, we're interested in how we can move from an EVM state to another one. There is the Ethereum virtual machine state transition that basically explains how that happens. And what happens in the layer two rollups is that this state transition is being arithmetized, and then we compute the proof that the state transition happened properly. And why this is useful is that not all nodes will have to perform the state transition themselves. They can just verify the proof. And since verification will be by design a very cheap operation, the overall system is going to be quite efficient. And as I already prepared you, we're going to look into how to construct such a proof system. There will be two parts in our talk. The first part will be the polynomial commitment scheme, which we call Vortex. And the second part will be how to use this polynomial commitment scheme, some other machinery, in order to reason about the VM execution. So let's dip into cryptography with polynomial commitments So basically like these two parts are the main building blocks of how to build a snark so polynomial commitments first you're going to have just a setup algorithm and At a very high level is just going to give you some public parameters so public parameters and if you want to commit you're going to use these public parameters you're going to use some polynomial you want to commit to a polynomial this polynomial will have some bounded degree K and you're going to output some commitments C and then we want to reason about this polynomial, about this commitment that we just made. And basically, we're going to look at the protocol that's interactive between a prover and a verifier. They both have access to the polynomial, to the parameters, to the commitment. And in the case of the prover also, yeah, also the polynomial, the verifier doesn't know the polynomial, only knows the commitment. And the prover basically attempts to prove that P of X is indeed equal to Y. And this happens over multiple interaction routes. Very high level, the security condition that we want to achieve is that the prover should not be able to convince the verifier in the case that he committed to something, to some polynomial that actually does not evaluate to the claimed Y. And we want to do this efficiently. We want to amortize our costs. So basically we're going to do something called batch polynomial commitments in which instead of committing to only one polynomial, we're going to commit to N of them. We're going to commit to n of them. We're going to claim n evaluations. And basically, the proof is going to be a batch proof, a combined proof, that all of these polynomials evaluate properly. And also, the verifier now knows the evaluation, the claimed evaluations. And let's look into the math that we use in order to build such a thing. These are called the Reed-Solomon codes. And without getting into the nitty gritty, we're going to use a finite field, f. If you don't know what a finite field is, just imagine integers, modulus, and prime p. And we are going to take some subset of n elements a1 to an from this finite field then encoding a polynomial so this is about the codes encoding a polynomial will just be evaluated the polynomial at this endpoints and in order for you to actually in order for this encoding to actually express information about the polynomial you want that the degree of P is actually smaller or equal than N in order to actually be able to express it uniquely. And in fact, you want to have, this N is going to be much higher, you want to have redundant information about the polynomial in order to error correct. And now let's look at the starting point where we took inspiration for our work, which is the Ligero breakdown protocol. These are two separate schemes, but they have a very common structure. So in this previous work, the setup algorithm would give you a hash function as the public parameters. And then in order to commit to n polynomials, what you're going to do is that you're going to first compute the encodings, the Reed-Solomon encodings of the n polynomials. Each row now corresponds to an encoding. You're going to construct this matrix, and you're going to apply a hash function on each column in order to get H1. Then you apply the hash function on the second column to get H2 and so on and this is going to be your commitment your commitment is these hashes of the columns of the encodings of your polynomials so that's how you commit and I put this here in case you forget it. Now we are so interested in how do we open, how do we actually prove to the verifier that our claimed evaluations are correct in a secure way. So basically the verifier is going to send the coin beta to the prover. The prover is going to compute, this looks like a complicated equation, but actually it's not. It's just you have each row, and each row gets multiplied with the power of beta. So it's a linear combination of the rows with this beta that comes from the verifier. This is a vector u. Send it to the verifier. The verifier now picks an index that it wants to check. It's only going to be able to check one. Send it back. And the prover replies to the column of that specific index in this case this is the first one the verifier does two checks it checks that indeed when it applies the hash function to the opened column to the one that we just revealed it is equal to the one in the commitment and it checks that indeed if you compute the linear combination of the betas with the revealed columns then you're going to get the ith coordinate of u. And this protocol is going to ensure that all the rows are indeed codewords. But we want more than this. We want to actually make a proof about the claimed evaluations of the polynomial, not just show that the rows are codewords. And what we notice in this work is that if we interpolate this vector that is sent by the prover to obtain some polynomial, and then we also check that this polynomial evaluated at x is equal to the linear combinations of the y's, if we do this additional check, we can prove that the polynomials evaluate properly and this proof can be very simple if you are in the unique decoding regime which I'm just going to talk about next the point is that if you want to opt it if you want optimal parameters the proof becomes very complicated and I'm going to give you just a flavor of why that is. So basically, when you're talking about codes, you can consider two cases. There is the unique decoding, which in this case is exemplified by this radius. This is the target vector that we want to decode. And this is a radius around it. And as you can see, if you set it this length, then there is only one code word. However, you can also consider relaxing this parameter to be able to include multiple points that are candidates for your decoding. And the proof for our polynomial commitments can become much more complicated if you want the optimal parameters, if you want to have something that's efficient in practice. So as I said, the small one is the unique decoding regime, there's only one candidate and the yellow one is the least one, multiple candidates. So the security guarantee in the that we show in the least decoding regime is that there will exist polynomials of small degree that evaluate correctly and these commitments will have only a small amount of coordinates that are different from the target commitment that we are decoding. And if you want more details for the proof of security, you can check out our ePrint. Thanks a lot. Now we're going to move to the second part of the talk, where we're looking at how can we use this polynomial commitment in conjunction with other things to reason about the EVM and I'm handing over to my colleague Azam. Thank you Bogdan. So let's go to the second part. How and here we are focusing what is happening on layer two, linear, and how we generate proof from EVM execution via such polynomial commitment. There are a lot of steps until we can generate proof. The first step in this journey is arithmetization. And arithmetization for us is mathematical modeling of EVM by columns and constraints between columns. For example, if you want to prove that a transaction had a valid signature, first you hash the signature, so you have to prove that the hash was correct. So if I want to here to show that hash of is why I start with column X and column X is a kind of describing your input X for example you can consider the byte of the X which are splitted in the cells of these columns then which with each steps of the computation that is happening in the hash, this column, you would have a new column with new values in the cell and so on until that you would arrive to column Y, which is your output. And there is constraint between this column to say that how the computation of the hash is going on so by this arithmetization here you get bunch of columns and constraint between columns there are different constraints for example it's just example here there are more lookup constraints or query we also can say which says column R is included in column B or local constraint that says okay if you have two columns then the first row of these two columns are equal global cond constraints, which are very important for us, and they are more about cells, the relation over the cells of the same column or even between several columns. I'm putting an example here for Fibonacci sequence. For this Fibonacci sequence, we know that for each cell is in fact addition of two previous cells. So you write the constraint like this. And we call this one trace, trace of execution or Fibonacci. Okay, if you want to verify this constraint as a verifier, definitely you don't want to do it like this because it's too many efforts, especially global constraints because they are over the cells constraint as a verifier. Definitely you don't want to do it like this because it's too many effort especially global constraint because they are over the cells and you have to check a lot of relation between columns and cells. That's where polynomial commitment comes to rescue. We write each column as a polynomial. We interpret the column as a polynomial. For example you can say that okay the values in the columns you can see them as the coefficient of a polynomial and by this the constraint would be translated as a relation between polynomials. And from here, you can apply polynomial commitment. I remind you that polynomial commitment can do two tasks for you, which are very simple but very important. One task was it can commit by a short commitment, and you can use the commitment to open to request the evaluation of the polynomial. Another interesting property of the polynomials that why we are interested in polynomial and why we are going from columns to polynomials is thanks to this lemma Schwartz-Dipper lemma, and it says that if you have a relation for a polynomial, between polynomial commitments, you don't need to check the relation for each x, you just take a random point from the domain and if the relation over the random point alpha is satisfied, then you know that it was satisfied for every x. This would simplify everything and that's why we are interested in polynomials. Because by this verifier task is very easy. It just needs to take a random point and then verify the polynomial and constraint over this random point. So by this, we can get now polynomials, a bunch of polynomials and constraints. We are very close to get the proof but we are not still there and the reason is that this constraint are not the constraint that I want. The constraint that we are interested in are global constraints. Why global constraints? Because thanks to this Schwarz-Zippel lemma you can verify them easily. Global constraints are between cells so you don't need to check them between cells, you use a short ZipLM over a global constraint, and you just check in a random point. So we are interested to kind of reduce this constraint. Every constraint that you have, for example, lookup queries, permutation queries, and a lot of queries that may happen during arithmetization, we want to replace them with global connoisseur because from there we can have a simple verification via Schwarz-Zippel lemma. To do this, to get global connoisseur, we need PIOP. But what is PIOP? PIOP stands for polynomial interactive oracle proof. It's the ideal modeling of a protocol between prover and verifier. You have a third party between prover and verifier, this handsome guy that we call Oracle. Oracle is honest and can answer any question. So prover and verifier start their interactions, and they use this Oracle to help them to reduce their constraint to global constraint. And there is a theorem that says that a polynomial commitment in such construction resembles to an oracle, because oracle does not exist. Such guy does not exist, unfortunately. So we have to replace it with something real. And there is a theorem that says that polynomial commitment is a good candidate and you can replace it with this guy to help you to reduce your polynomials to constraint to normal to global and now we are fine so we use P IOP you reduce everything to the global constraint and we can use a polynomial commitment for this. But we cannot use R-Vortex. And the reason is that R-Vortex is not a standard polynomial commitment and the theorem is not true. It's not a good candidate for Oracle. Why it's not a good candidate? It's because of this property, least property. Why it makes problem? Why it cannot work with PIOP? And the reason is that in PIOP you have rounds of interactions between prover and verifier and in fact due to this least property polynomial has a lot of choices in different rounds. And he can do kind of mix-match attack between different rounds and different polynomials from the list. But we have a trick for this. We can prevent, in fact, we use a trick to force, which we call it commit separately and open collectively. And by this, I don't go to the details, theoretical details, but by this trick, we are able to force the prover, in fact, to open all the polynomial commitments at the same position over the disk, let's say. So for the first round, maybe he has choices, but for the other rounds, he doesn't have choice, and he has to stick to the first choice. So this would solve the problem of list, but still, vortex is not good. One more step. And why it's not good? It's now because this noise property, I call it noise because it's batching, but batching, it's not a general batching. As you see in vortex, the batching is just a batching of all the polynomials over the same point. And in the real application, PIOP applications, when you applied your PIOP, in fact, you are in the general case that you have many polynomials, many points. So this kind of batching, even though it's nice, but it's more fantasy and it's not applicable here. But we also find another theoretical trick to solve this problem. Still, I would not go to the theory, just the general view is that we come back to the step that we designed our PIOP, and we kind of modify it to be able to use a batching over the same point and the way that we modify it is just first the convert can speak freely and at the end we apply a kind of reduction via Oracle over the same point and from here now vortex is applicable we can replace our vortex with Oracle and we are done finally we have our first proof and here is just an example of the parameters if you are interested thank you very much. I love the dynamic between the two of you explaining the different sides. It was cool. We have some questions and also some that I had. Sorry, I also introduced you wrong. I said you're from the prover team of ConsenSys. And then I was like, ConsenSys doesn't do proofs. So from the Linnea team, and I guess that became clear immediately. But yeah, so one of the first questions was, is this the vortex that Linnea uses? Yes, this is the first proof. That's why I mentioned that finally we have our first proof. This is the first proof that we generate. After, there are a lot of more layers that will come to aggregate the these initial proofs and finally a final proof to be compatible with aetherium but this is yes this is the first proof that we generate in linear and the classic any benchmarks because as I said this is the first proof. So we have several kind of recursion. And our estimation is that you need, for 100 bits of security, you need 15 millions of constraints for the verifier of vortex inside of Planck. And comparing to Breakdown and Ligero that are in unique decoding, this is four times fast, not faster, but the size of the proof is four times smaller. Nice. So I think this question comes up, well this might answer the next question in part at least. So what are the advantages, disadvantages of Vortex compared to other polynomial commitment schemes? So in terms of prover time, proof size, you mentioned the proof size already. Yes, I mentioned about proof size that is faster than breakdown, smaller than breakdown four times due to the least commitment property. And regarding the prover time, both are good. I mean, it's as good as breakdown, but it's also, I can compare it with fry, for example, because Vortex, it's a very important property that it doesn't need trusted setup. It's like Frye. And comparing to Frye, the prover is faster. Yeah. Good. So, a last question. Is batching over the same point, does it impact the security? That's why we went all through this modifying POP and modifying this trick of commit separately open collectively. All of this was for the proof of security so the paper in fact analyzed the security that, it's not a standard commitment, so how can we use it securely? Nice. I see maybe one more question came in as we have two minutes left for questions. Maybe we can scroll and see. I actually left my phone. Okay. Never mind. What question would you love to hear? Pick your own question. What field do you use? In fact, the only property that we need is because we use FFT, so the only property that we need is to add city, which means that the order of a if your field is a Q the prime that you are using Q minus Y should be a big factor of 2 so if you have this property it's enough for us after because we also have used it's a lattice for the hash, for hashing of the columns. So we may, for optimization, we have more choices in distance. But already this fact that we just need two addicity, it would give us a big choice. Nice. Good. And thank you to whoever uploaded the question so we could read it. Collaboration at its finest. So let's thank the speakers once again. Thank you. And yeah, catch them if you want to know about Linnaeus Provers. Thank you very much. Bye.",
  "eventId": "devcon-7",
  "slot_start": 1731409200000,
  "slot_end": 1731411000000,
  "slot_roomId": "stage-3",
  "resources_presentation": "https://docs.google.com/presentation/d/1xTWn8OHn3Uo4DFB83e-yHN6sTgrIz_lfqTY2XGPrx98",
  "resources_slides": null,
  "speakers": [
    "azam-soleimanian",
    "bogdan-ursu"
  ]
}